{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Pretrained Language Model\n",
    "\n",
    "This notebook demonstrates using a pretrained model from Hugging Face Transformers (e.g., DistilBERT) for inference or fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# For example, we'll use 'distilbert-base-uncased-finetuned-sst-2-english' for sentiment classification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    positive_prob = probs[0,1].item()\n",
    "    return positive_prob\n",
    "\n",
    "sample_text = \"I love using Transformers for NLP!\"\n",
    "print(\"Text:\", sample_text)\n",
    "print(\"Positive Sentiment Probability:\", predict_sentiment(sample_text))\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "name": "pretrained_language_model",
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
