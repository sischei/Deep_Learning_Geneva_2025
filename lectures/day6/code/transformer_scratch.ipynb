{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n",
    "This notebook shows a simplified implementation of a Transformer model from scratch, focusing on the encoder portion and self-attention. Weâ€™ll use toy data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by num_heads\"\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_size = x.shape\n",
    "\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Reshape Q, K, V for multi-head attention\n",
    "        Q = Q.view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Permute for correct matrix multiplication: (N, heads, seq_len, head_dim)\n",
    "        Q = Q.permute(0, 2, 1, 3)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, V)  # (N, heads, seq_length, head_dim)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        out = out.view(N, seq_length, self.embed_size)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attention(x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        forward_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(forward_out))\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Toy example\n",
    "    embed_size = 32\n",
    "    num_heads = 4\n",
    "    forward_expansion = 4\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Create a random input: batch_size=2, seq_len=10\n",
    "    x = torch.rand((2, 10, embed_size))\n",
    "\n",
    "    encoder_block = TransformerEncoderBlock(embed_size, num_heads, forward_expansion, dropout)\n",
    "    out = encoder_block(x)\n",
    "    print(out.shape)  # Expected: [2, 10, 32]\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "name": "transformer_scratch",
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
