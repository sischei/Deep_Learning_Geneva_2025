{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Transformer on a Toy Classification Task\n",
    "\n",
    "We'll use a small dataset (e.g., IMDB-like sentiment) and a simpler Transformer architecture.\n",
    "This notebook demonstrates the steps for fine-tuning: data loading, tokenization, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "# For simplicity, we could use a built-in dataset from torchtext\n",
    "# but torchtext API changes frequently. We'll assume older torchtext.\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy', lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# Build vocab (in a real case, much larger)\n",
    "TEXT.build_vocab(train_data, max_size=10000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# Minimal Transformer-based classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, output_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(5000, embed_dim)  # assume max seq len = 5000 for simplicity\n",
    "\n",
    "        encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder_layers.append(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout\n",
    "                )\n",
    "            )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.Sequential(*encoder_layers), num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [seq_len, batch_size]\n",
    "        seq_len, batch_size = text.shape\n",
    "        positions = (torch.arange(0, seq_len).unsqueeze(1).expand(seq_len, batch_size)).to(text.device)\n",
    "\n",
    "        embedded = self.embedding(text) + self.pos_embedding(positions)\n",
    "        # embedded shape: [seq_len, batch_size, embed_dim]\n",
    "\n",
    "        # Transformer in PyTorch expects [sequence_length, batch_size, embedding_dim]\n",
    "        transformer_out = self.transformer_encoder(embedded)\n",
    "        # We'll take the mean of the sequence outputs as a 'sentence embedding'\n",
    "        pooled = transformer_out.mean(dim=0)\n",
    "        return self.fc(self.dropout(pooled))\n",
    "\n",
    "# Initialize model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 1  # binary classification\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = TransformerClassifier(INPUT_DIM, EMBED_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "# Training loop (simplified)\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_iterator):.4f}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "name": "finetune_transformer",
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
