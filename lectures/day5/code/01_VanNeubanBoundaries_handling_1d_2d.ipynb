{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cc3c7a-732b-4038-910b-bd42b5fb02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 57.96015167236328\n",
      "Epoch 500, Loss: 0.0036900572013109922\n",
      "Epoch 1000, Loss: 0.0003550474939402193\n",
      "Epoch 1500, Loss: 0.000316944089718163\n",
      "Epoch 2000, Loss: 0.000299551960779354\n",
      "Epoch 2500, Loss: 0.00028349546482786536\n",
      "Epoch 3000, Loss: 0.00026441249065101147\n",
      "Epoch 3500, Loss: 0.00024170932010747492\n",
      "Epoch 4000, Loss: 0.00021538091823458672\n",
      "Epoch 4500, Loss: 0.0001862893404904753\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "PINN for 1D ODE with Neumann Boundary Conditions\n",
    "===============================================================================\n",
    "This example demonstrates a Physics-Informed Neural Network (PINN) to solve a\n",
    "simple one-dimensional ordinary differential equation (ODE) with Neumann boundary\n",
    "conditions.\n",
    "\n",
    "Problem Setup:\n",
    "---------------\n",
    "We consider the ODE:\n",
    "    \n",
    "    u''(x) + π² sin(π x) = 0,   x ∈ [0, 1]\n",
    "\n",
    "whose exact solution is given by\n",
    "\n",
    "    u(x) = sin(π x).\n",
    "\n",
    "Accordingly, the derivative is\n",
    "\n",
    "    u'(x) = π cos(π x).\n",
    "\n",
    "Thus, the Neumann boundary conditions are:\n",
    "    - At x = 0: u'(0) = π cos(0) = π,\n",
    "    - At x = 1: u'(1) = π cos(π) = -π.\n",
    "\n",
    "Boundary Condition Enforcement:\n",
    "-------------------------------\n",
    "There are two common approaches in PINNs:\n",
    "1. Hard enforcement – building the trial solution to inherently satisfy the boundary\n",
    "   conditions (often via output transformations).\n",
    "2. Soft enforcement – adding penalty terms in the loss function that measure the\n",
    "   discrepancy at the boundaries.\n",
    "\n",
    "In this code we use soft enforcement. The Neumann conditions are enforced by:\n",
    "   • Computing the network output u(x) at the boundary points.\n",
    "   • Using automatic differentiation to obtain u'(x) at x = 0 and x = 1.\n",
    "   • Penalizing the squared difference between these computed derivatives and\n",
    "     the prescribed values (π at x = 0 and –π at x = 1).\n",
    "\n",
    "The total loss function is the sum of:\n",
    "   • The residual loss – enforcing that u(x) satisfies the differential equation\n",
    "     at collocation points in the domain.\n",
    "   • The boundary loss – enforcing that the derivative u'(x) at the boundaries\n",
    "     matches the Neumann conditions.\n",
    "\n",
    "This code uses PyTorch for building the network and performing automatic differentiation.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple fully-connected neural network for the PINN\n",
    "class PINN1D(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN1D, self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        layer_list = []\n",
    "        for i in range(len(layers)-1):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.layers = nn.ModuleList(layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (N,1)\n",
    "        out = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.layers[-1](out)\n",
    "        return out\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Generate collocation points in the domain [0, 1]\n",
    "N_collocation = 1000\n",
    "x_collocation = torch.linspace(0, 1, N_collocation).view(-1, 1)\n",
    "# Enable gradient tracking for automatic differentiation\n",
    "x_collocation.requires_grad = True\n",
    "\n",
    "# Define boundary points for Neumann BC (here we use one point at each boundary)\n",
    "x_bc = torch.tensor([[0.0], [1.0]], requires_grad=True)\n",
    "# Prescribed derivative values computed from u'(x)=π cos(πx)\n",
    "bc_values = torch.tensor([[np.pi], [-np.pi]], dtype=torch.float32)\n",
    "\n",
    "# Instantiate the neural network: input layer (1 neuron), three hidden layers, output layer (1 neuron)\n",
    "layers = [1, 20, 20, 20, 1]\n",
    "model = PINN1D(layers)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_iter = 5000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # PDE Residual Loss Computation\n",
    "    # ---------------------------\n",
    "    # Predict u(x) at collocation points\n",
    "    u = model(x_collocation)\n",
    "    # Compute first derivative u_x = du/dx\n",
    "    u_x = torch.autograd.grad(u, x_collocation, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    # Compute second derivative u_xx = d²u/dx²\n",
    "    u_xx = torch.autograd.grad(u_x, x_collocation, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    # Define the ODE residual: u''(x) + π² sin(πx) = 0\n",
    "    f = torch.sin(np.pi * x_collocation)\n",
    "    residual = u_xx + (np.pi**2) * f\n",
    "    loss_res = torch.mean(residual**2)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Boundary Loss for Neumann BC (Soft Enforcement)\n",
    "    # ---------------------------\n",
    "    # Evaluate network at the boundary points\n",
    "    u_bc = model(x_bc)\n",
    "    # Compute derivative u'(x) at the boundaries\n",
    "    u_bc_x = torch.autograd.grad(u_bc, x_bc, grad_outputs=torch.ones_like(u_bc), create_graph=True)[0]\n",
    "    # Mean squared error between computed derivative and prescribed BC values\n",
    "    loss_bc = torch.mean((u_bc_x - bc_values)**2)\n",
    "    \n",
    "    # Total loss is a sum of PDE residual and boundary losses\n",
    "    loss = loss_res + loss_bc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fa9840-9e02-459c-bf7b-4beb21be8bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 106.51937103271484\n",
      "Epoch 500, Loss: 0.12774918973445892\n",
      "Epoch 1000, Loss: 0.02215679921209812\n",
      "Epoch 1500, Loss: 0.010023819282650948\n",
      "Epoch 2000, Loss: 0.00548941595479846\n",
      "Epoch 2500, Loss: 0.003494677832350135\n",
      "Epoch 3000, Loss: 0.0021805635187774897\n",
      "Epoch 3500, Loss: 0.0015068539651110768\n",
      "Epoch 4000, Loss: 0.0011680236784741282\n",
      "Epoch 4500, Loss: 0.000977365649305284\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "PINN for 2D PDE with Neumann Boundary Conditions\n",
    "===============================================================================\n",
    "This example demonstrates a Physics-Informed Neural Network (PINN) to solve a\n",
    "two-dimensional partial differential equation (PDE) with Neumann boundary conditions.\n",
    "\n",
    "Problem Setup:\n",
    "---------------\n",
    "We consider the PDE:\n",
    "\n",
    "    u_xx(x,y) + u_yy(x,y) + 2π² sin(π x) cos(π y) = 0,   for (x,y) ∈ [0,1]×[0,1]\n",
    "\n",
    "The exact solution is given by\n",
    "\n",
    "    u(x,y) = sin(π x) cos(π y).\n",
    "\n",
    "The corresponding derivatives are:\n",
    "    u_x(x,y) = π cos(π x) cos(π y),\n",
    "    u_y(x,y) = -π sin(π x) sin(π y).\n",
    "\n",
    "Thus, the Neumann boundary conditions are:\n",
    "    - On x = 0: u_x(0,y) = π cos(π y),\n",
    "    - On x = 1: u_x(1,y) = -π cos(π y),\n",
    "    - On y = 0 and y = 1: u_y(x,y) = 0  (since sin(0)=0 and sin(π)=0).\n",
    "\n",
    "Boundary Condition Enforcement:\n",
    "-------------------------------\n",
    "As in the 1D case, we use soft enforcement. For each boundary:\n",
    "   • We sample points on the boundary.\n",
    "   • We compute the network output u(x,y) at those points.\n",
    "   • Using automatic differentiation, we compute the appropriate normal derivative:\n",
    "       - For vertical boundaries (x=0 and x=1), we compute u_x.\n",
    "       - For horizontal boundaries (y=0 and y=1), we compute u_y.\n",
    "   • We then form a penalty (mean squared error) between the computed derivative and\n",
    "     the target Neumann value prescribed by the analytical solution.\n",
    "     \n",
    "The overall loss is the sum of:\n",
    "   • The PDE residual loss (ensuring that the PDE is approximately satisfied in the\n",
    "     interior of the domain).\n",
    "   • The boundary loss (ensuring that the Neumann boundary conditions hold).\n",
    "\n",
    "This code uses PyTorch for both the neural network and for computing derivatives.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define a fully-connected neural network for the 2D PINN\n",
    "class PINN2D(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN2D, self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        layer_list = []\n",
    "        for i in range(len(layers)-1):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.layers = nn.ModuleList(layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (N,2) where columns correspond to (x,y)\n",
    "        out = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.layers[-1](out)\n",
    "        return out\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ---------------------------\n",
    "# Generate Interior Collocation Points in [0,1]x[0,1]\n",
    "# ---------------------------\n",
    "N_collocation = 10000\n",
    "n_side = int(np.sqrt(N_collocation))\n",
    "x_coll = torch.linspace(0, 1, n_side)\n",
    "y_coll = torch.linspace(0, 1, n_side)\n",
    "X, Y = torch.meshgrid(x_coll, y_coll, indexing='ij')\n",
    "# Reshape and concatenate to obtain interior points\n",
    "X_interior = torch.cat((X.reshape(-1, 1), Y.reshape(-1, 1)), dim=1)\n",
    "X_interior.requires_grad = True\n",
    "\n",
    "# ---------------------------\n",
    "# Generate Boundary Points for Neumann BC\n",
    "# ---------------------------\n",
    "N_bc = 200  # Number of points per boundary segment\n",
    "\n",
    "# Left boundary (x = 0): sample y uniformly\n",
    "y_bc_left = torch.rand((N_bc, 1))\n",
    "x_bc_left = torch.zeros((N_bc, 1))\n",
    "bc_left = torch.cat((x_bc_left, y_bc_left), dim=1)\n",
    "bc_left.requires_grad = True\n",
    "\n",
    "# Right boundary (x = 1): sample y uniformly\n",
    "y_bc_right = torch.rand((N_bc, 1))\n",
    "x_bc_right = torch.ones((N_bc, 1))\n",
    "bc_right = torch.cat((x_bc_right, y_bc_right), dim=1)\n",
    "bc_right.requires_grad = True\n",
    "\n",
    "# Bottom boundary (y = 0): sample x uniformly\n",
    "x_bc_bottom = torch.rand((N_bc, 1))\n",
    "y_bc_bottom = torch.zeros((N_bc, 1))\n",
    "bc_bottom = torch.cat((x_bc_bottom, y_bc_bottom), dim=1)\n",
    "bc_bottom.requires_grad = True\n",
    "\n",
    "# Top boundary (y = 1): sample x uniformly\n",
    "x_bc_top = torch.rand((N_bc, 1))\n",
    "y_bc_top = torch.ones((N_bc, 1))\n",
    "bc_top = torch.cat((x_bc_top, y_bc_top), dim=1)\n",
    "bc_top.requires_grad = True\n",
    "\n",
    "# ---------------------------\n",
    "# Define Target Boundary Values from the Analytical Solution:\n",
    "# u(x,y) = sin(πx) cos(πy)\n",
    "# Therefore:\n",
    "#   For x-boundaries: u_x = π cos(πx) cos(πy)\n",
    "#       At x=0: u_x(0,y) = π cos(0) cos(πy) = π cos(πy)\n",
    "#       At x=1: u_x(1,y) = π cos(π) cos(πy) = -π cos(πy)\n",
    "#   For y-boundaries: u_y = -π sin(πx) sin(πy)\n",
    "#       At y=0 and y=1: u_y = 0 (since sin(0)=0 or sin(π)=0)\n",
    "# ---------------------------\n",
    "def bc_left_target(y):\n",
    "    return np.pi * torch.cos(np.pi * y)\n",
    "\n",
    "def bc_right_target(y):\n",
    "    return -np.pi * torch.cos(np.pi * y)\n",
    "\n",
    "def bc_bottom_target(x):\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "def bc_top_target(x):\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Instantiate the Neural Network\n",
    "# ---------------------------\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "model = PINN2D(layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_iter = 5000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # PDE Residual Loss Computation\n",
    "    # ---------------------------\n",
    "    u = model(X_interior)\n",
    "    # Compute gradients with respect to x and y\n",
    "    grads = torch.autograd.grad(u, X_interior, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = grads[:, 0:1]\n",
    "    u_y = grads[:, 1:2]\n",
    "    # Compute second derivatives: u_xx and u_yy\n",
    "    u_xx = torch.autograd.grad(u_x, X_interior, grad_outputs=torch.ones_like(u_x), create_graph=True)[0][:, 0:1]\n",
    "    u_yy = torch.autograd.grad(u_y, X_interior, grad_outputs=torch.ones_like(u_y), create_graph=True)[0][:, 1:2]\n",
    "    # The PDE is: u_xx + u_yy + 2π² sin(πx) cos(πy) = 0\n",
    "    f = torch.sin(np.pi * X_interior[:, 0:1]) * torch.cos(np.pi * X_interior[:, 1:2])\n",
    "    residual = u_xx + u_yy + 2 * (np.pi**2) * f\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Boundary Loss Computation (Soft Enforcement of Neumann BC)\n",
    "    # ---------------------------\n",
    "    # Left boundary (x=0): enforce u_x(0,y)= π cos(πy)\n",
    "    u_left = model(bc_left)\n",
    "    u_left_x = torch.autograd.grad(u_left, bc_left, grad_outputs=torch.ones_like(u_left), create_graph=True)[0][:, 0:1]\n",
    "    target_left = bc_left_target(bc_left[:, 1:2])\n",
    "    loss_left = torch.mean((u_left_x - target_left)**2)\n",
    "    \n",
    "    # Right boundary (x=1): enforce u_x(1,y)= -π cos(πy)\n",
    "    u_right = model(bc_right)\n",
    "    u_right_x = torch.autograd.grad(u_right, bc_right, grad_outputs=torch.ones_like(u_right), create_graph=True)[0][:, 0:1]\n",
    "    target_right = bc_right_target(bc_right[:, 1:2])\n",
    "    loss_right = torch.mean((u_right_x - target_right)**2)\n",
    "    \n",
    "    # Bottom boundary (y=0): enforce u_y(x,0)= 0\n",
    "    u_bottom = model(bc_bottom)\n",
    "    u_bottom_y = torch.autograd.grad(u_bottom, bc_bottom, grad_outputs=torch.ones_like(u_bottom), create_graph=True)[0][:, 1:2]\n",
    "    target_bottom = bc_bottom_target(bc_bottom[:, 0:1])\n",
    "    loss_bottom = torch.mean((u_bottom_y - target_bottom)**2)\n",
    "    \n",
    "    # Top boundary (y=1): enforce u_y(x,1)= 0\n",
    "    u_top = model(bc_top)\n",
    "    u_top_y = torch.autograd.grad(u_top, bc_top, grad_outputs=torch.ones_like(u_top), create_graph=True)[0][:, 1:2]\n",
    "    target_top = bc_top_target(bc_top[:, 0:1])\n",
    "    loss_top = torch.mean((u_top_y - target_top)**2)\n",
    "    \n",
    "    loss_bc = loss_left + loss_right + loss_bottom + loss_top\n",
    "    \n",
    "    # Total loss: sum of interior PDE residual loss and boundary loss\n",
    "    loss = loss_pde + loss_bc\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7cc9b0-3775-4106-9158-e9ce35fe8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 48.6347541809082\n",
      "Epoch 500, Loss: 11.553899765014648\n",
      "Epoch 1000, Loss: 5.396996021270752\n",
      "Epoch 1500, Loss: 2.965437889099121\n",
      "Epoch 2000, Loss: 1.7772620916366577\n",
      "Epoch 2500, Loss: 1.1364892721176147\n",
      "Epoch 3000, Loss: 0.7607347369194031\n",
      "Epoch 3500, Loss: 0.5289739966392517\n",
      "Epoch 4000, Loss: 0.37436825037002563\n",
      "Epoch 4500, Loss: 0.27223652601242065\n",
      "Training finished - Hard Enforcement (1D)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "PINN for 1D ODE with Neumann Boundary Conditions - Hard Enforcement\n",
    "===============================================================================\n",
    "This example demonstrates a Physics-Informed Neural Network (PINN) to solve a\n",
    "one-dimensional ordinary differential equation (ODE) with Neumann boundary\n",
    "conditions using hard enforcement.\n",
    "\n",
    "Problem Setup:\n",
    "---------------\n",
    "We consider the ODE:\n",
    "\n",
    "    u''(x) + π² sin(π x) = 0,   x ∈ [0, 1]\n",
    "\n",
    "with the exact solution\n",
    "\n",
    "    u(x) = sin(π x).\n",
    "\n",
    "Thus, the derivative is\n",
    "\n",
    "    u'(x) = π cos(π x),\n",
    "\n",
    "and the Neumann conditions are:\n",
    "    - u'(0) = π,\n",
    "    - u'(1) = -π.\n",
    "\n",
    "Hard Enforcement Strategy:\n",
    "--------------------------\n",
    "Instead of penalizing the boundary derivative in the loss (soft enforcement),\n",
    "we build the trial solution as:\n",
    "\n",
    "    u_trial(x) = g(x) + h(x)*N(x),\n",
    "\n",
    "where:\n",
    "    • g(x) is a predetermined function that exactly satisfies the BCs:\n",
    "          Let g(x) = π x - π x².\n",
    "          Then, g'(0)=π and g'(1)=π-2π = -π.\n",
    "    • h(x) is a function that vanishes in its derivative at the boundaries:\n",
    "          Let h(x) = x²*(1-x)².\n",
    "    • N(x) is the neural network output.\n",
    "Thus, no matter what N(x) is, the derivative of u_trial(x) at the boundaries is:\n",
    "    u_trial'(x) = g'(x) + [h'(x)*N(x) + h(x)*N'(x)],\n",
    "and since h'(0)=h'(1)=0, we have\n",
    "    u_trial'(0)=g'(0)=π,  u_trial'(1)=g'(1)=-π.\n",
    "\n",
    "The loss function is then solely based on the PDE residual in the domain.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network for the correction term N(x)\n",
    "class PINN1D(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN1D, self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        layer_list = []\n",
    "        for i in range(len(layers)-1):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (N,1)\n",
    "        out = x\n",
    "        for layer in self.net[:-1]:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.net[-1](out)\n",
    "        return out\n",
    "\n",
    "# Functions for hard enforcement\n",
    "def g(x):\n",
    "    # g(x) = π x - π x²\n",
    "    return np.pi * x - np.pi * x**2\n",
    "\n",
    "def h(x):\n",
    "    # h(x) = x²*(1-x)²\n",
    "    return x**2 * (1 - x)**2\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Generate collocation points in the domain [0, 1]\n",
    "N_collocation = 1000\n",
    "x_collocation = torch.linspace(0, 1, N_collocation).view(-1, 1)\n",
    "x_collocation.requires_grad = True\n",
    "\n",
    "# Instantiate the neural network: input layer (1 neuron), hidden layers, output layer (1 neuron)\n",
    "layers = [1, 20, 20, 20, 1]\n",
    "model = PINN1D(layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_iter = 5000\n",
    "\n",
    "# Training loop for hard enforcement: only the PDE residual is penalized.\n",
    "for epoch in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the network output N(x)\n",
    "    N_pred = model(x_collocation)\n",
    "    # Compute the trial solution: u(x) = g(x) + h(x)*N(x)\n",
    "    x_np = x_collocation.detach().cpu().numpy()\n",
    "    g_val = torch.tensor(g(x_np), dtype=torch.float32)\n",
    "    h_val = torch.tensor(h(x_np), dtype=torch.float32)\n",
    "    # Ensure same shape as x_collocation\n",
    "    g_val = g_val.view(-1, 1)\n",
    "    h_val = h_val.view(-1, 1)\n",
    "    u = g_val + h_val * N_pred\n",
    "    \n",
    "    # Compute derivatives: u_x and u_xx\n",
    "    u_x = torch.autograd.grad(u, x_collocation, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x_collocation, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    \n",
    "    # PDE residual: u''(x) + π² sin(πx) = 0\n",
    "    f = torch.sin(np.pi * x_collocation)\n",
    "    residual = u_xx + (np.pi**2) * f\n",
    "    loss_res = torch.mean(residual**2)\n",
    "    \n",
    "    loss = loss_res\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished - Hard Enforcement (1D)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5649febe-68e8-4e77-a65a-46aa26147172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 97.39935302734375\n",
      "Epoch 500, Loss: 92.94125366210938\n",
      "Epoch 1000, Loss: 89.18375396728516\n",
      "Epoch 1500, Loss: 67.68165588378906\n",
      "Epoch 2000, Loss: 60.44888687133789\n",
      "Epoch 2500, Loss: 55.34098815917969\n",
      "Epoch 3000, Loss: 49.651947021484375\n",
      "Epoch 3500, Loss: 41.922855377197266\n",
      "Epoch 4000, Loss: 40.552467346191406\n",
      "Epoch 4500, Loss: 31.22797203063965\n",
      "Training finished - Hard Enforcement (2D)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "PINN for 2D PDE with Neumann Boundary Conditions - Hard Enforcement\n",
    "===============================================================================\n",
    "This example demonstrates a Physics-Informed Neural Network (PINN) to solve a\n",
    "two-dimensional partial differential equation (PDE) with Neumann boundary\n",
    "conditions using hard enforcement.\n",
    "\n",
    "Problem Setup:\n",
    "---------------\n",
    "We consider the PDE:\n",
    "\n",
    "    u_xx(x,y) + u_yy(x,y) + 2π² sin(πx) cos(πy) = 0,   for (x,y) ∈ [0,1]×[0,1]\n",
    "\n",
    "with the exact solution\n",
    "\n",
    "    u(x,y) = sin(πx) cos(πy).\n",
    "\n",
    "The derivatives are:\n",
    "    u_x(x,y) = π cos(πx) cos(πy),\n",
    "    u_y(x,y) = -π sin(πx) sin(πy).\n",
    "\n",
    "Thus, the Neumann boundary conditions are:\n",
    "    - On x = 0: u_x(0,y) = π cos(πy),\n",
    "    - On x = 1: u_x(1,y) = -π cos(πy),\n",
    "    - On y = 0: u_y(x,0) = 0,\n",
    "    - On y = 1: u_y(x,1) = 0.\n",
    "\n",
    "Hard Enforcement Strategy:\n",
    "--------------------------\n",
    "We build the trial solution as:\n",
    "\n",
    "    u_trial(x,y) = g(x,y) + h(x,y) * N(x,y),\n",
    "\n",
    "where:\n",
    "    • g(x,y) is chosen to satisfy the boundary derivative conditions:\n",
    "          Let g(x,y)= π x cos(πy) - π x² cos(πy).\n",
    "          Then, g_x(0,y)= π cos(πy) and g_x(1,y)= -π cos(πy);\n",
    "          Also, g_y(x,0)=g_y(x,1)=0.\n",
    "    • h(x,y) is chosen such that its normal derivatives vanish on all boundaries:\n",
    "          A convenient choice is h(x,y)= x²(1-x)² * y²(1-y)².\n",
    "    • N(x,y) is the neural network output.\n",
    "Because h(x,y) is constructed so that its derivatives vanish along the boundaries,\n",
    "the normal derivative of u_trial is u_trial_x=g_x and u_trial_y=g_y at the respective\n",
    "boundaries—thus automatically satisfying the Neumann conditions.\n",
    "\n",
    "The loss function is solely the interior PDE residual.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network for the correction term N(x,y)\n",
    "class PINN2D(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN2D, self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        layer_list = []\n",
    "        for i in range(len(layers)-1):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (N,2) where columns correspond to (x,y)\n",
    "        out = x\n",
    "        for layer in self.net[:-1]:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.net[-1](out)\n",
    "        return out\n",
    "\n",
    "# Hard enforcement functions for 2D\n",
    "def g(xy):\n",
    "    # g(x,y)= π x cos(πy) - π x² cos(πy)\n",
    "    x = xy[:, 0:1]\n",
    "    y = xy[:, 1:2]\n",
    "    return np.pi * x * np.cos(np.pi * y) - np.pi * (x**2) * np.cos(np.pi * y)\n",
    "\n",
    "def h(xy):\n",
    "    # h(x,y)= x²*(1-x)² * y²*(1-y)²\n",
    "    x = xy[:, 0:1]\n",
    "    y = xy[:, 1:2]\n",
    "    return (x**2 * (1 - x)**2) * (y**2 * (1 - y)**2)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ---------------------------\n",
    "# Generate Interior Collocation Points in [0,1]x[0,1]\n",
    "# ---------------------------\n",
    "N_collocation = 10000\n",
    "n_side = int(np.sqrt(N_collocation))\n",
    "x_coll = torch.linspace(0, 1, n_side)\n",
    "y_coll = torch.linspace(0, 1, n_side)\n",
    "X, Y = torch.meshgrid(x_coll, y_coll, indexing='ij')\n",
    "X_interior = torch.cat((X.reshape(-1, 1), Y.reshape(-1, 1)), dim=1)\n",
    "X_interior.requires_grad = True\n",
    "\n",
    "# Instantiate the neural network\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "model = PINN2D(layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_iter = 5000\n",
    "\n",
    "# Training loop for hard enforcement: only the PDE residual is penalized.\n",
    "for epoch in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute network output N(x,y)\n",
    "    N_pred = model(X_interior)\n",
    "    \n",
    "    # Compute g(x,y) and h(x,y) on interior points\n",
    "    xy_np = X_interior.detach().cpu().numpy()\n",
    "    g_val = torch.tensor(g(xy_np), dtype=torch.float32)\n",
    "    h_val = torch.tensor(h(xy_np), dtype=torch.float32)\n",
    "    g_val = g_val.view(-1, 1)\n",
    "    h_val = h_val.view(-1, 1)\n",
    "    \n",
    "    # Compute trial solution: u(x,y)= g(x,y) + h(x,y)*N(x,y)\n",
    "    u = g_val + h_val * N_pred\n",
    "    \n",
    "    # Compute gradients with respect to x and y\n",
    "    grads = torch.autograd.grad(u, X_interior, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = grads[:, 0:1]\n",
    "    u_y = grads[:, 1:2]\n",
    "    # Second derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, X_interior, grad_outputs=torch.ones_like(u_x), create_graph=True)[0][:, 0:1]\n",
    "    u_yy = torch.autograd.grad(u_y, X_interior, grad_outputs=torch.ones_like(u_y), create_graph=True)[0][:, 1:2]\n",
    "    \n",
    "    # PDE residual: u_xx + u_yy + 2π² sin(πx) cos(πy) = 0\n",
    "    f = torch.sin(np.pi * X_interior[:, 0:1]) * torch.cos(np.pi * X_interior[:, 1:2])\n",
    "    residual = u_xx + u_yy + 2 * (np.pi**2) * f\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    \n",
    "    loss = loss_pde\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished - Hard Enforcement (2D)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db735bf-517f-4bc0-94e8-f1727a59660c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
