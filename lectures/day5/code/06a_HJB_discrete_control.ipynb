{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181e8519-1b0d-4275-93d1-b8836564fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 0, Loss: 6.910488e-01\n",
      "Epoch 500, Loss: 3.313205e-01\n",
      "Epoch 1000, Loss: 3.293123e-01\n",
      "Epoch 1500, Loss: 3.315267e-01\n",
      "Epoch 2000, Loss: 3.370330e-01\n",
      "Epoch 2500, Loss: 3.309459e-01\n",
      "Epoch 3000, Loss: 3.321407e-01\n",
      "Epoch 3500, Loss: 3.334538e-01\n",
      "Epoch 4000, Loss: 3.412174e-01\n",
      "Epoch 4500, Loss: 3.336690e-01\n",
      "HJB training complete. Evaluate or refine for your specific control sets.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Title: Simple 1D Hamilton–Jacobi–Bellman (HJB) Equation using PINNs\n",
    "\n",
    "We solve a toy HJB:\n",
    "    -r*V(x) + max_{a in [-1,1]} [ (x+a)*V'(x) - alpha*a^2 ] = 0\n",
    "on x in [0,1], with boundary conditions:\n",
    "    V(0) = 0,\n",
    "    V(1) = 1.\n",
    "\n",
    "We approximate the \"max\" over a discrete set of controls in [-1,1].\n",
    "PINN approach:\n",
    "    - A small network V(x).\n",
    "    - Residual = -r*V + max_{a in [-1,1]} [ (x+a)*V'(x) - alpha*a^2 ].\n",
    "    - Then add boundary constraints.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "r = 0.05\n",
    "alpha = 0.1  # penalty coefficient for a^2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class HJBNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural net for V(x).\n",
    "    We'll keep it simple: input x -> some hidden layers -> output V.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=20):\n",
    "        super(HJBNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, hidden_units)\n",
    "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer3 = nn.Linear(hidden_units, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.layer1(x))\n",
    "        out = self.activation(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "def hjb_residual(model, x):\n",
    "    \"\"\"\n",
    "    HJB PDE residual:\n",
    "      -r*V(x) + max_{a in [-1,1]} [ (x+a)*V'(x) - alpha*a^2 ] = 0.\n",
    "    We'll approximate max by sampling discrete a values, e.g. a in {-1, -0.5, 0, 0.5, 1}.\n",
    "    \"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    V = model(x)\n",
    "    dVdx = torch.autograd.grad(\n",
    "        V, x,\n",
    "        grad_outputs=torch.ones_like(V),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Sample discrete controls in [-1,1].\n",
    "    A = torch.linspace(-1.0, 1.0, steps=5).to(device)  # e.g., -1, -0.5, 0, 0.5, 1\n",
    "\n",
    "    # Evaluate expression for each a\n",
    "    values = []\n",
    "    for a in A:\n",
    "        # expression = (x+a)*V'(x) - alpha*a^2\n",
    "        expr = (x + a)*dVdx - alpha*(a**2)\n",
    "        values.append(expr)\n",
    "\n",
    "    # Combine across a dimension => shape (N, 5)\n",
    "    stacked_vals = torch.stack(values, dim=-1)\n",
    "    # We approximate the max over a by taking the maximum along dim=-1\n",
    "    max_expr, _ = torch.max(stacked_vals, dim=-1, keepdim=True)\n",
    "\n",
    "    # PDE residual = -r*V + max_expr\n",
    "    res = -r*V + max_expr\n",
    "    return res\n",
    "\n",
    "def main():\n",
    "    model = HJBNet(hidden_units=20).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 5000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sample interior points\n",
    "        x_interior = torch.rand((100,1), device=device)  # x in (0,1)\n",
    "\n",
    "        # PDE residual\n",
    "        res = hjb_residual(model, x_interior)\n",
    "        loss_pde = torch.mean(res**2)\n",
    "\n",
    "        # Boundary conditions: V(0)=0, V(1)=1\n",
    "        x0 = torch.tensor([[0.0]], device=device)\n",
    "        x1 = torch.tensor([[1.0]], device=device)\n",
    "        V0 = model(x0)\n",
    "        V1 = model(x1)\n",
    "        loss_bc = torch.mean((V0 - 0.0)**2) + torch.mean((V1 - 1.0)**2)\n",
    "\n",
    "        loss = loss_pde + loss_bc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6e}\")\n",
    "\n",
    "    print(\"HJB training complete. Evaluate or refine for your specific control sets.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72788f30-989a-4d88-8a73-e9ca938454ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
